

{
    "start-title": "The beginning of the journey",
    "start-content": "I have a Bachelor's degree in Accounting Sciences from the Federal University of Pernambuco. Throughout my career I assumed the position of administrative manager, where I developed solid skills in financial analysis, team management and optimization of business processes. This experience gave me a deep understanding of how data can drive strategic decisions and operational improvements.",
    "the-way-title": "The Way",
    "the-way-content-1": "In recent years, I discovered a new passion: data analysis. I have dedicated myself to learning languages programming skills such as Python, SQL, as well as familiarizing myself with data visualization tools such as Tableau and Power BI. My accounting experience gave me a solid foundation for interpreting and analyzing data with precision and attention to detail.",
    "the-way-content-2": "The transition to data analysis is a challenge I'm looking forward to taking on. I love the idea of ​​combining my accounting and administrative knowledge with advanced data analysis techniques to generate insights valuable resources and make informed decisions. I believe that my ability to communicate complex information clear and accessible way will be a great asset on this new journey.",
    "the-way-content-3": "I am determined to advance further and become a Data Scientist. In this role, I intend to delve deeper my knowledge in Machine Learning, statistics and predictive analysis to solve complex problems and develop advanced models that can predict trends and optimize processes.",
    "The-ultimate-goal-title": "The ultimate goal",
    "The-ultimate-goal-content": "Finally, my dream is to become an Artificial Intelligence Engineer. I see artificial intelligence as the final frontier of data, where I can apply everything I've learned to create innovative solutions and intelligent. I am committed to continuous learning and adapting to rapid technological changes, and I believe that my passion for solving problems and my multifaceted experience will prepare me well for this future.",
    "banner-resume": "My accounting experience has given me a strong foundation in financial analysis, attention to detail and data-driven decision making. As administrative manager, I developed skills in leadership, team management and process optimization. These skills are complementary to Data Science, where I intend to apply my skills. knowledge for solving business problems, extracting insights meaningful and support strategic decision-making motivated by constant evolution from the market.",
    "analytical-thinking-title": "Analytical Thinking",
    "analytical-thinking-content": "This skill allows me to face challenges in a structured and efficient way, contributing to my ability to solve problems creatively and find innovative solutions.",
    "continuous-learning-title": "Continuous Learning",
    "continuous-learning-content": "I am always looking for new knowledge and experiences, whether through reading, research, interacting with others, or exploring. I see every opportunity as a chance for personal growth and development.",
    "comunication-title": "Comunication",
    "comunication-content": "I am skilled at expressing my ideas clearly and concisely, adapting my communication style according to the context and audience. I am a good listener capable of articulating my own ideas confidently and persuasively.",
    "sql-title": "SQL",
    "sql-content": "I have a solid understanding of the fundamental principles of the language and can work independently on most database-related tasks through data joining, filtering, and sorting operations with confidence.",
    "python-title": "Python",
    "python-content": "I am comfortable with Python syntax, including control flow structures, data types, functions, and data manipulation. Additionally, I have experience using popular libraries such as NumPy, Pandas, Matplotlib and Seaborn for data analysis and visualization",
    "dataviz-title": "DataViz",
    "dataviz-content": "I have knowledge of how to choose the correct type of chart to represent different types of data, as well as how to apply formatting, colors and labels to make visualizations more understandable and attractive through storytelling using Power BI, Tableu, Looker Studio.",
    "side-bar-main-projects": "Main Projects",
    "button-see": "See project",
    "objective-definition": "Objective definition",
    "collect-extract": "Data Collection and Extraction",
    "choice-tools": "Choice of Tools",
    "techniques": "Application of analysis techniques",
    "results": "Results",
    "tools": "Tools",
    "related-projects": "Related Projects",
    "projects-title": "Projects",

    
    "amz-index": "Descriptive analysis on prices, discounts and evaluation of products sold in the Indian division of the Amazon, applying statistics seeking to observe variations.",
    "amz-bi-page-theme": "We used data from the Indian branch of the retail giant to carry out a descriptive analysis of how products are distributed by category, evaluations, price, discount and correlation between evaluation and product value.",
    "amz-obj": "We will analyze a public dataset available on Kaggle on sales data from Amazon's Indian branch.",
    "amz-tech-1": "Cleaning and Pre-processing",
    "amz-tech-2": "Exploratory Data Analysis",
    "amz-tech-3": "Descriptive Analysis",
    "amz-results-1": "We use some Cards to View Total Products, Overall Average Rating, Largest Number of Reviews on a single product and Overall Total Ratings.",
    "amz-results-2": "Using a Horizontal Bar Chart to understand the distribution of products by category.",
    "amz-results-3": "We highlight the 3 Product Categories with the Most Reviews using the Pie Chart.",
    "amz-results-4": "We evaluate the Average Number of Product Reviews divided by Categories.",
    "amz-results-5": "On this page we highlight some indicators:",
    "amz-results-6": "Highest Absolute Discount Value",
    "amz-results-7": "Lowest Value Product - Mini USB Type C Adapter Plug",
    "amz-results-8": "Higher Percentage Discount Applied",
    "amz-results-9": "Higher Percentage Discount Applied",
    "amz-results-10": "We use bar graphs to analyze the Absolute and Percentage Discount Averages by Category and Product.",
    "amz-results-11": "We segment products by range by price, discount and evaluation.",
    "amz-results-12": "We can see in the scatter plot that there is a slightly positive correlation between the Total Ratings and the Rating of the analyzed products.",
    "amz-results-13": "We can see in the scatter plot that there is a slightly positive correlation between the Final Price of discounted products and the Ratings of the analyzed products.",
    "amz-results-14": "We can see in the scatter plot that there is a slightly negative correlation between the Product Discount Percentage and the Reviews of the analyzed products.",
    "amz-results-15": "Page used to report possible Dashboard updates.",

    "data-index": "EDA using Pandas and Seaborn to analyze data on the salaries of professionals in the field at a global level.",
    "data-obj": "We will analyze a set of public data available on Kaggle about the Salaries of Data Area professionals.",
    "data-bi-page-theme": "In this project we will delve deeper into how the remuneration of data professionals is configured at a Global level. We will use geographic segmentation, level of experience, work model, company size, among others.",
    "data-tech-1": "Cleaning and Pre-processing;",
    "data-tech-2": "Exploratory Data Analysis;",
    "data-tech-3": "Descriptive Analysis;",
    "data-results-1": "On the main page we highlight the average total salary, as well as maximum, minimum values ​​and number of different professions;",
    "data-results-2": "Using the retractable filter, we can segment the data according to experience level and category;",
    "data-results-3": "On this page, using a table, we list all the professions described in the research;",
    "data-results-4": "Using horizontal bar graphs, we segment the number of professionals according to the type of work and the size of the company they are associated with;",
    "data-results-5": "On the last page the objective was to show the global distribution;",
    "data-results-6": "We can use the filter to segment by Continent, Country, Employee Residence and Company Location;",
    "data-results-7": "We were also able to visualize the number of each professional according to experience and their average salary;",

    "olist-index": "In this project, a Data Warehouse was built through SQL Server using data from Olist, a Brazilian Ecommerce platform that connects the retailer to the end consumer.",
    "olist-da-page-theme": "We built a small Data Warehouse project using data provided by Olist, a Brazilian ecommerce startup that connects sellers and logists to customers. SQL Server was used for data modeling, Figma for building the background and Microsoft Power BI for assembling the Dashboard.",
    "techniques-olist": "Application of Modeling/Cleaning/Imputation techniques",
    "olist-obj-1": "In this project we will use the Brazilian ecommerce public data set, made available by Olist, a Startup that operates in the retail technology segment;",
    "olist-obj-2": "Olist connects small businesses in Brazil to communication channels so that merchants can sell their products via the web and ship them directly to customers using logistics partners;",
    "olist-obj-3": "The data was obtained from the Kaggle platform in CSV format, we used SQL Server 2022 as the DBMS and SQL Server Management Studio to carry out the necessary queries;",
    "olist-obj-4": "The first step was to configure the necessary instance on SQL Server;",
    "olist-obj-5": "Then we started importing the CSV files;",
    "olist-obj-6": "We create the necessary tables to better select and manage the data needed for the project;",
    "olist-tech-SQL-title": "SQL - Modeling/Cleaning/Imputation",
    "olist-tech-1": "We note that the same prefix can contain different latitudes and longitudes, always indicating the same city and state, that is, the prefix it is used in a generic way because it is incomplete so we cannot associate the exact location such as neighborhood and street, therefore we can keep only 1 prefix value as it will only indicate the city and state;",
    "olist-tech-1.1": "This will help reduce the number of records and consequently the loading and connection time between data in Power BI, which cannot recognize the relationship between tables because there is duplication;",
    "olist-tech-2": "We will create the Dimension and Fact Tables, with their respective Natural Keys and Surrogate Keys for DataWarehouse Dimensional Modeling;",
    "olist-tech-3": "We will create a reusable Function to replace state acronyms with full names, as the abbreviation can be linked to states in other countries;",
    "olist-tech-4": "We will create a calendar dimension table in DW for better temporal analysis in Power BI;",
    "olist-tech-dax-title": "DAX - Measures/Functions",
    "olist-tech-5": "Creation of Measures (CALCULATE,SUM,AVERAGE,COUNT,DISTINCTCOUNT) to use in generating results;",
    "olist-tech-6": "Use of the CROSSFILTER function to link dimension tables and avoid the use of both directions, which can generate ambiguity;",
    "olist-tech-powerquery-title": "Power Query - Replacement/Transformation",
    "olist-tech-7": "Replacement of values ​​in the calendar dimension table for Day of the Week and Month in English;",
    "olist-tech-8": "Transformation of values ​​in the product category column to remove the '_' and raise the first character to Capitalize in each first letter of the words;",
    "olist-tech-9": "Raising the first character of each word in the cities column to Capitalize;",
    "olist-tech-10": "Correction of records that can be added to the same city;",
    "olist-model": "This is what the Data Warehouse Dimensional Model looked like in Star Schema.",
    "olist-describe": "A descriptive table of Olist data was created and can be accessed by clicking on the logo.",
    "olist-filter": "Results can be segmented by state using the retractable Filter Panel.",
    "olist-results-page-main": "Main Page",
    "olist-results-1": "Total Number of Customers - Card;",
    "olist-results-2": "Total Number of Sellers - Card;",
    "olist-results-3": "General Rating Average - Card;",
    "olist-results-4": "Total Quantity of Products - Card;",
    "olist-results-5": "Total Quantity of Categories - Card;",
    "olist-results-6": "Table 1 - Number of Orders by Status;",
    "olist-results-7": "Table 2 - Average Rating by Category;",
    "olist-results-8": "Chart 1 - Horizontal Bar Chart Total Number of Customers per City;",
    "olist-results-9": "Chart 2 - Horizontal Bar Chart Total Number of Customers by State;",
    "olist-results-10": "Chart 3 - Vertical Bar Chart Number of Orders by Payment Type;",
    "olist-results-page-finance": "Finance Page",
    "olist-results-11": "Average Order Value - Card;",
    "olist-results-12": "Higher Value Order - Card;",
    "olist-results-13": "Total Revenue - Card;",
    "olist-results-14": "Table - Total Revenue by Status Type;",
    "olist-results-15": "Chart 4 - Funnel Rank States with the highest Revenue;",
    "olist-results-16": "Chart 5 - Bar Chart Total Revenue by Payment Type;",
    "olist-results-17": "Chart 6 - Horizontal Bar Chart Revenue Rank by Product Category;",
    "olist-results-page-historical": "Historical Page",
    "olist-results-18": "Chart 7 - Line Chart Concentration of purchasing volume (Day of the Week);",
    "olist-results-19": "Chart 8 - Area Chart Concentration of purchase volume (Day);",
    "olist-results-20": "Chart 9 - Area Chart Concentration of purchasing volume (Time);",
    "olist-results-21": "Chart 10 - Area Chart Concentration of purchase volume (Month);",
    "olist-filter-historical": "On this page the results can also be filtered by year;",

    "foodgood-index": "Using ETL techniques, I created a Data Warehouse to load through the SQL Server Integration Service and perform descriptive analysis (EDA) of the Brazilian delivery platform Delivery Center.",
    "foodgood-da-page-theme": "In this Data Warehouse project I used SSIS to load data from the Relational Bank hosted in SQL Server to the Data Warehouse of a Delivery platform responsible for sales of Food and Consumer Goods, we will be able to analyze the abusive, regional and financial numbers of orders, stores and cities where points of sale and distribution centers are located.",
    "describe-foodgood-title": "Project Description",
    "foodgood-describe-1": "The Delivery Center is a platform that integrates logistics and marketplaces, creating a logistics ecosystem that allows the sale of consumer goods (GOOD) and food (FOOD) in Brazilian retail.",
    "foodgood-describe-2": "This model represents fictitious data on orders and deliveries that were processed between the months of January and April 2021, it does not represent its completeness and some data has been changed to preserve customers in accordance with the LGPD",
    "foodgood-describe-3": "Its distribution centers (hubs) were located in shopping malls in some capitals in Brazil, in this data set data from Curitiba, Porto Alegre, Rio de Janeiro and São Paulo were provided.",
    "foodgood-describe-4": "The company founded in 2016 achieved relative success throughout its history, especially during the COVID-19 pandemic in 2020, but closed its operations in 2021 after conflicts between shareholders, fierce competition and an abrupt drop in revenue.",
    "foodgood-obj-1": "In this Data Warehouse we will synthesize the data in order to obtain insights into absolute quantities, regional distribution and the company's financial behavior during the analyzed period;",
    "foodgood-obj-2": "We will use the dataset to create a Data Warehouse through SQL Server Integration Services (SSIS);",
    "source": "Data Source",
    "source-describe-1": "channels: information relating to the sales channels where the retailers' Good and Food;",
    "source-describe-2": "deliveries: data about deliveries;",
    "source-describe-3": "drivers: data about delivery people;",
    "source-describe-4": "hubs: information about the hubs;",
    "source-describe-5": "orders: data on orders placed;",
    "source-describe-6": "payments: information about payments made;",
    "source-describe-7": "stores: data about the stores that use the platform;", 
    "foodgood-SSIS": "Data Warehouse Modeling",
    "foodgood-tech-1": "First we will insert the CSV data obtained from Kaggle into SQL Server",
    "foodgood-tech-2": "Then, after connecting SSIS to SQL Server, we will simulate a data load from the relational Database created in SQL Server to the Data Warehouse that will be created at the end of each task in Data Flow, some Transformation steps (Conversion, Derived Column and Conditional Division) were used to standardize data types, Imputation of missing values ​​and Removal of Outliers, therefore an example of how the data was loaded into the payments table using OLE DB as follows:",
    "foodgood-tech-3": "The Data Flow of data loading from dimension tables:",
    "foodgood-tech-4": "The second step will be to load the Fact table:",
    "foodgood-tech-5": "Lastly, we will automate the data loading flow:",
    "foodgood-tech-6": "This is what the Data Warehouse Dimensional Model will look like:",
    "foodgood-tech-7": "Below you will find the SQL script for querying the Relational Database and creating the Data Warehouse:",
    "foodgood-describe": "Power BI was used to visualize the data with customization of the background with Figma based on UI/UX and Storytelling elements.",
    "foodgood-cleaning": "In PowerQuery we apply the necessary transformations for data cleaning, conversion, imputation, outlier removal and attribute engineering.",
    "foodgood-filter": "A retractable filter was created to use data segmentation.",
    "foodgood-results-page-home": "On the first page I carried out an analysis of the absolute numbers to obtain insights into how orders are distributed according to the following parameters:",
    "foodgood-results-1": "Sales by day of the week we can see that there is a seasonality where the majority of sales, 49% of the total 358,654 made, occur at the weekend.",
    "foodgood-results-2": "87% of total absolute sales are represented by the Food segment.",
    "foodgood-results-3": "68% of deliveries are carried out by Freelance workers with no employment relationship with the logisticians.",
    "foodgood-results-4": "265 thousand deliveries were made by motorcycles, representing 70% of total deliveries.",
    "foodgood-results-5": "The months of April and March represent 60% of sales, with emphasis on March where there were 109 thousand orders placed.",
    "foodgood-results-6": "Most orders were delivered, totaling 98% of the total.",
    "foodgood-results-7": "The Delivery Center platform was responsible for 90% of orders placed, only 10% of logists used their own applications.",
    "foodgood-results-page-region": "On this page the objective was to carry out an analysis of the cities in which the stores are located and how the average delivery time is behaving, the absolute number of stores and sales per store and also the financial representation of the stores.",
    "foodgood-results-8": "951 stores were counted, in 32 different hubs, located in 4 Brazilian capitals.",
    "foodgood-results-9": "On this card I observed a large discrepancy in relation to the average delivery time, investigating further I found orders that took up to 4 months to be completed, these outliers pull the average mainly from FOOD in the city of São Paulo, obtaining an average time of 1210 minutes , that is, 20 hours.",
    "foodgood-results-10": "In terms of absolute numbers of orders, São Paulo was responsible for 45% of requests, followed by Rio de Janeiro with 38% of requests.",
    "foodgood-results-11": "The Golden Shopping Hub located in the city of Rio de Janeiro was the champion in the total value of orders, representing 10.7% of the total, however 4 of the next 5 positions are from Hubs located in São Paulo, thus showing us that in an absolute way São Paulo, in addition to the number of orders, is most responsible for the value of the data set.",
    "foodgood-results-page-finance": "On the Finance page, the objective was to analyze in detail how the financial numbers of the data set behaved, such as order value, cost, refunds made, average value, behavior over time and the most used payment methods.",
    "foodgood-results-12": "The first card shows us that the amount of orders was approximately R$37.2M while the delivery cost was approximately R$3M, the refund value was R$7,161 with a very low representation.",
    "foodgood-results-13": "The average order value was R$92.8 after carrying out the Outlier treatment that eliminated a single order worth R$100K, on ​​the last card I presented the average value of the fee charged R$1.88.",
    "foodgood-results-14": "Observing the graph representing the amount and cost of delivery over time, it is possible to see that it coincides with the graph showing the number of orders on the home page, where we have relative parity in January and February followed by growth in March and returning to decrease in April.",
    "foodgood-results-15": "In terms of payment method, we generally have the online method, however, it is important to note that this is a generic description, as some of the other payment methods can also be carried out online, such as credit card and bank transfer, so it is possible This category refers to payments made through the platform and not at the time of delivery or collection.",
    "foodgood-results-final": "The data set used in the construction of the Data Warehouse allowed us to carry out important analyzes of the company's absolute, regional and financial behavior at a time of economic recovery after the COVID-19 pandemic. So that future analyzes could better understand the metrics related to delivery drivers, such as the number of deliveries made, average delivery time, distance traveled."
}
